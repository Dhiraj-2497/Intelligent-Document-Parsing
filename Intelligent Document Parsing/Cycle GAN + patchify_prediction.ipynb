{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"L-AzzeCrVZL2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683616040366,"user_tz":-330,"elapsed":46882,"user":{"displayName":"Dhiraj Hasija","userId":"16227978329846526818"}},"outputId":"c04d0457-6f98-44d0-9933-935b25ba036c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-vkxn915v\n","  Running command git clone --filter=blob:none --quiet https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-vkxn915v\n","  warning: redirecting to https://github.com/keras-team/keras-contrib.git/\n","  Resolved https://www.github.com/keras-team/keras-contrib.git to commit 3fc5ef709e061416f4bc8a92ca3750c824b5d2b0\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-contrib==2.0.8) (2.12.0)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101078 sha256=46b3239e83897bf3399bf9ca73015e750818f167cfd343f5e014497f6b49acb0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-uxnpd_32/wheels/74/d5/f7/0245af7ac33d5b0c2e095688649916e4bf9a8d6b3362a849f5\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n","Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting patchify\n","  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from patchify) (1.22.4)\n","Installing collected packages: patchify\n","Successfully installed patchify-0.2.3\n"]}],"source":["from keras.models import load_model\n","from os import listdir\n","from numpy import asarray, vstack\n","from tensorflow.keras.utils import img_to_array, load_img\n","from numpy import savez_compressed\n","import numpy as np\n","from PIL import Image\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from matplotlib import pyplot\n","!pip install git+https://www.github.com/keras-team/keras-contrib.git\n","from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install patchify\n","from patchify import patchify, unpatchify"]},{"cell_type":"code","source":["import os\n","import time\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import torch.nn.functional as F"],"metadata":{"id":"puvyjZV--ec3","executionInfo":{"status":"ok","timestamp":1683616044478,"user_tz":-330,"elapsed":4115,"user":{"displayName":"Dhiraj Hasija","userId":"16227978329846526818"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# class Convnet(nn.Module):\n","    \n","#     def __init__(self):\n","#         super().__init__()\n","#         self.conv1 = nn.Conv2d(3,32,kernel_size=3,bias=False)\n","#         self.batch1 = nn.BatchNorm2d(32)\n","        \n","#         self.conv2 = nn.Conv2d(32,64,kernel_size=3,bias=False)\n","#         self.batch2 = nn.BatchNorm2d(64)\n","        \n","#         self.conv3 = nn.Conv2d(64,128,kernel_size=3,bias=False)\n","#         self.batch3 = nn.BatchNorm2d(128)\n","        \n","#         self.conv4 = nn.Conv2d(128,512,kernel_size=3,bias=False)\n","#         self.batch4 = nn.BatchNorm2d(512)\n","        \n","# #         self.conv5 = nn.Conv2d(512,1048,kernel_size=4,stride=2,padding=1,bias=False)\n","# #         self.batch5 = nn.BatchNorm2d(1048)\n","        \n","        \n","#         self.linear1=nn.Linear(512*14*14,1048,bias=False)\n","#         self.batch_l1=nn.BatchNorm1d(1048)\n","        \n","#         self.linear2=nn.Linear(1048,512,bias=False)\n","#         self.batch_l2=nn.BatchNorm1d(512)\n","        \n","#         self.linear3=nn.Linear(512,256,bias=False)\n","#         self.batch_l3=nn.BatchNorm1d(256)\n","        \n","#         self.linear4=nn.Linear(256,1,bias=False)\n","        \n","        \n","#         self.pool = nn.MaxPool2d(2,2)\n","#         self.drop = nn.Dropout(0.1)\n","        \n","#     def forward(self,x):\n","    \n","        \n","#         x=self.pool(F.leaky_relu(self.conv1(x),0.2))\n","#         x=self.batch1(x)\n","        \n","#         x=self.pool(F.leaky_relu(self.conv2(x),0.2))\n","#         x=self.batch2(x)\n","        \n","#         x=self.pool(F.leaky_relu(self.conv3(x),0.2))\n","#         x=self.batch3(x)\n","        \n","#         x=self.pool(F.leaky_relu(self.conv4(x),0.2))\n","#         x=self.batch4(x)\n","        \n","# #         x=self.pool(F.relu(self.conv5(x)))\n","# #         x=self.batch5(x)\n","\n","        \n","        \n","#         x=x.view(-1,512*14*14)\n","  \n","        \n","#         x=self.drop(F.leaky_relu(self.linear1(x),0.2))\n","#         x=self.batch_l1(x)\n","        \n","#         x=self.drop(F.leaky_relu(self.linear2(x),0.2))\n","#         x=self.batch_l2(x)\n","        \n","#         x=self.drop(F.leaky_relu(self.linear3(x),0.2))\n","#         x=self.batch_l3(x)\n","        \n","#         x=self.drop(F.leaky_relu(self.linear4(x),0.2))\n","\n","        \n","#         return torch.sigmoid(x)"],"metadata":{"id":"9vsgjzjB-jt2","executionInfo":{"status":"ok","timestamp":1683559764885,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dhiraj Hasija","userId":"16227978329846526818"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# net=Convnet()\n","# net.load_state_dict(torch.load('/content/drive/Shareddrives/IDP/Cycle GAN Implentation/Watermark/Text Classifier/PatchClassifier.pt',map_location=torch.device('cpu')))"],"metadata":{"id":"FejnZ6oNvh4R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras\n","clsfr = keras.models.load_model(\"/content/drive/Shareddrives/IDP/Cycle GAN Implentation/2. Motion_blur_dataset_Jan_23/Patches filtered/MB_text_blank_classifier.h5\")\n"],"metadata":{"id":"OnjdVoQd9I8I","executionInfo":{"status":"ok","timestamp":1683616051609,"user_tz":-330,"elapsed":7134,"user":{"displayName":"Dhiraj Hasija","userId":"16227978329846526818"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def summarize_performance(a, step, g_model, trainX, name, n_samples=1):\n","\tX_in = trainX\n","\tX_in = (X_in - 127.5)/127.5\n","\tX_out = g_model.predict(trainX, verbose = 0)\n","\tX_out = (X_out + 1)/2\n","\treturn X_out"],"metadata":{"id":"7vPsYgQO-rvu","executionInfo":{"status":"ok","timestamp":1683616051610,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dhiraj Hasija","userId":"16227978329846526818"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# net.eval()\n","# with torch.no_grad():\n","\n","Folder = '/content/drive/Shareddrives/IDP/Cycle GAN Implentation/5. MB_Blur_Testing_Results/v_40_best_2_models_prediction'\n","model_list = []\n","folder = Folder + '/Models/'\n","for i in listdir(folder):\n","  model_list.append(folder + i)\n","print(model_list)\n","\n","a = []\n","for i in [5, 20, 24, 26, 28]:\n","  _ = 'S_Img_Android_D' + str(i) + '_L2_r35_a0_b0_Mb1.jpg'\n","  a.append(_)\n","\n","for i in [2,17,26]:\n","  _ = 'S_Img_Android_D' + str(i) + '_L2_r35_a0_b0_Mb2.jpg'\n","  a.append(_)\n","\n","print(a)\n","\n","for model in model_list:\n","  g_model1 = load_model(model,custom_objects={'InstanceNormalization':InstanceNormalization})\n","  for k in range(len(a)):\n","    filename = '/content/drive/Shareddrives/IDP/VisionAPI/IMAGES/SmartdocQA/' + a[k]\n","    image = Image.open(f\"{filename}\")\n","    image = image.rotate(-90)\n","    image = np.asarray(image)\n","    output_shape = image.shape\n","    patches = patchify(image, (256, 256, 3), step=256)\n","    print(patches.shape)\n","    output_patches = np.empty(patches.shape).astype(np.uint8)\n","\n","    for i in range(patches.shape[0]):\n","      for j in range(patches.shape[1]):\n","        num = i * patches.shape[1] + j\n","        patch1=patches[i, j, 0].reshape(1,256,256,3)\n","        # patch1 = patch.reshape(1,256,256,3)\n","        p = clsfr.predict(patch1, verbose = 0)[0][0]\n","\n","        if p < 0.5:\n","          #print('predicting')\n","          pred = 255*summarize_performance(f\"{'M_Img_Android_D21_L3_r35_a5_b10_Mb2'}patch_pred{num}.jpg\",1, g_model1, patch1, 'Mb2_blurB2A1', n_samples=1)\n","          pred = pred.astype(int)\n","        else:\n","          pred = patch1\n","          #print('keeping same')\n","        output_patches[i, j, 0] = pred\n","\n","    #from patchify import unpatchify\n","      image_height, image_width, channel_count = output_shape\n","      patch_height, patch_width, step = 256, 256, 256\n","\n","      output_height = image_height - (image_height - patch_height) % step\n","      output_width = image_width - (image_width - patch_width) % step\n","      output_shape = (output_height, output_width, channel_count)\n","\n","      output_image = unpatchify(output_patches, output_shape)\n","    #pyplot.imshow((output_image))\n","      output_image = Image.fromarray(output_image)\n","      output_image.save(Folder + '/Predicted_images/' + a[k].split('.')[0] + '_pred' + model.split('.')[1].split('_')[-1] +'.jpg')\n","\n","for k in range(len(a)):\n","  filename = '/content/drive/Shareddrives/IDP/VisionAPI/IMAGES/SmartdocQA/' + a[k]\n","  image = Image.open(f\"{filename}\")\n","  image = image.rotate(-90)\n","  image.save(Folder + '/Predicted_images/' + a[k])"],"metadata":{"id":"Fa-yA9dpRLi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k in range(len(a)):\n","  filename = '/content/drive/Shareddrives/IDP/VisionAPI/IMAGES/SmartdocQA/' + a[k]\n","  image = Image.open(f\"{filename}\")\n","  image = image.rotate(-90)\n","  image.save('/content/drive/Shareddrives/IDP/Cycle GAN Implentation/5. MB_Blur_Testing_Results/2_models_v_33/v33_pred_&_unfiltered/' + a[k])"],"metadata":{"id":"V55tOupF26Nx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/Top_6_model_pred_MB_v12.zip /content/Output"],"metadata":{"id":"HqwWebvziPAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Take best model and predict\n","\n","net.eval()\n","with torch.no_grad():\n","\n","#path to best model\n","  model_list = [\n","              '/content/drive/Shareddrives/IDP/Cycle GAN Implentation/4. All_blur_testing_Results/All Blur v12 Top 5 Models + all result patches/g_model_BtoA_001171.h5',\n","              '/content/drive/Shareddrives/IDP/Cycle GAN Implentation/5. MB_Blur_Testing_Results/Top_6_models_&_all_results_MB_only_v12/g_model_BtoA_001411.h5',\n","              '/content/drive/Shareddrives/IDP/Cycle GAN Implentation/6. OOF_Blur_Testing_Results/v10_All_results+Best models_OOF Jan 20/Final models/g_model_BtoA_004033.h5'\n","              ]\n","\n","  for model in model_list:\n","        g_model1 = load_model(model,custom_objects={'InstanceNormalization':InstanceNormalization})\n","        a = ['S_Img_Android_D1_L2_r35_a0_b0_Ob1.jpg',\n","             'S_Img_Android_D1_L2_r35_a0_b0_Ob2.jpg',\n","             'S_Img_Android_D1_L2_r35_a0_b0_Ob3.jpg',\n","             'S_Img_Android_D1_L2_r35_a0_b0_Ob4.jpg',\n","             'S_Img_Android_D1_L2_r35_a0_b0_Mb1.jpg',\n","             'S_Img_Android_D1_L2_r35_a0_b0_Mb2.jpg',\n","\n","             'S_Img_Android_D5_L2_r35_a0_b0_Ob1.jpg',\n","             'S_Img_Android_D5_L2_r35_a0_b0_Ob2.jpg',\n","             'S_Img_Android_D5_L2_r35_a0_b0_Ob3.jpg',\n","             'S_Img_Android_D5_L2_r35_a0_b0_Ob4.jpg',\n","             'S_Img_Android_D5_L2_r35_a0_b0_Mb1.jpg',\n","             'S_Img_Android_D5_L2_r35_a0_b0_Mb2.jpg',\n","\n","             'S_Img_Android_D13_L2_r35_a0_b0_Ob1.jpg',\n","             'S_Img_Android_D13_L2_r35_a0_b0_Ob2.jpg',\n","             'S_Img_Android_D13_L2_r35_a0_b0_Ob3.jpg',\n","             'S_Img_Android_D13_L2_r35_a0_b0_Ob4.jpg',\n","             'S_Img_Android_D13_L2_r35_a0_b0_Mb1.jpg',\n","             'S_Img_Android_D13_L2_r35_a0_b0_Mb2.jpg',\n","\n","             'S_Img_Android_D18_L2_r35_a0_b0_Ob1.jpg',\n","             'S_Img_Android_D18_L2_r35_a0_b0_Ob2.jpg',\n","             'S_Img_Android_D18_L2_r35_a0_b0_Ob3.jpg',\n","             'S_Img_Android_D18_L2_r35_a0_b0_Ob4.jpg',\n","             'S_Img_Android_D18_L2_r35_a0_b0_Mb1.jpg',\n","             'S_Img_Android_D18_L2_r35_a0_b0_Mb2.jpg',\n","             \n","             'S_Img_Android_D21_L2_r35_a0_b0_Ob1.jpg',\n","             'S_Img_Android_D21_L2_r35_a0_b0_Ob2.jpg',\n","             'S_Img_Android_D21_L2_r35_a0_b0_Ob3.jpg',\n","             'S_Img_Android_D21_L2_r35_a0_b0_Ob4.jpg',\n","             'S_Img_Android_D21_L2_r35_a0_b0_Mb1.jpg',\n","             'S_Img_Android_D21_L2_r35_a0_b0_Mb2.jpg',\n","\n","             'S_Img_Android_D25_L2_r35_a0_b0_Ob1.jpg',\n","             'S_Img_Android_D25_L2_r35_a0_b0_Ob2.jpg',\n","             'S_Img_Android_D25_L2_r35_a0_b0_Ob3.jpg',\n","             'S_Img_Android_D25_L2_r35_a0_b0_Ob4.jpg',\n","             'S_Img_Android_D25_L2_r35_a0_b0_Mb1.jpg',\n","             'S_Img_Android_D25_L2_r35_a0_b0_Mb2.jpg',\n","\n","             'S_Img_Android_D28_L2_r35_a0_b0_Ob1.jpg',\n","             'S_Img_Android_D28_L2_r35_a0_b0_Ob2.jpg',\n","             'S_Img_Android_D28_L2_r35_a0_b0_Ob3.jpg',\n","             'S_Img_Android_D28_L2_r35_a0_b0_Ob4.jpg',\n","             'S_Img_Android_D28_L2_r35_a0_b0_Mb1.jpg',\n","             'S_Img_Android_D28_L2_r35_a0_b0_Mb2.jpg',\n","             ]\n","\n","        for k in range(len(a)):\n","          filename = '/content/drive/Shareddrives/IDP/VisionAPI/IMAGES/SmartdocQA/' + a[k]\n","          image = Image.open(f\"{filename}\")\n","          image = image.rotate(-90)\n","          image = np.asarray(image)\n","          output_shape = image.shape\n","          patches = patchify(image, (256, 256, 3), step=256)\n","          print(patches.shape)\n","          output_patches = np.empty(patches.shape).astype(np.uint8)\n","          #fig, ax = plt.subplots(patches.shape[0], patches.shape[1],figsize=(20,20))\n","\n","          for i in range(patches.shape[0]):\n","              for j in range(patches.shape[1]):\n","                  #print(i,j)\n","                  num = i * patches.shape[1] + j\n","                  pixels=np.expand_dims(patches[i, j, 0], 0)\n","                  output=net(torch.tensor(pixels/255,dtype=torch.float32).reshape(1,3,256,256))\n","                  p = round(output.item(),2)\n","        \n","                  if p > 0.1:\n","                    print('predicting')\n","                    #print(pixels.max(),pixels.min())\n","                    pred = 255*summarize_performance(f\"{'M_Img_Android_D21_L3_r35_a5_b10_Mb2'}patch_pred{num}.jpg\",1, g_model1, pixels, 'Mb2_blurB2A1', n_samples=1)\n","                    pred = pred.astype(int)\n","                    #print(pred.max(),pred.min())\n","                  else:\n","                    pred = pixels\n","                    #print(pixels.max(),pixels.min())\n","                    print('keeping same')\n","                  output_patches[i, j, 0] = pred\n","\n","\n","          #from patchify import unpatchify\n","          image_height, image_width, channel_count = output_shape\n","          patch_height, patch_width, step = 256, 256, 256\n","\n","          output_height = image_height - (image_height - patch_height) % step\n","          output_width = image_width - (image_width - patch_width) % step\n","          output_shape = (output_height, output_width, channel_count)\n","\n","          output_image = unpatchify(output_patches, output_shape)\n","          #pyplot.imshow((output_image))\n","          output_image = Image.fromarray(output_image)\n","          output_image.save('/content/drive/Shareddrives/IDP/Cycle GAN Implentation/9. Final OCR Testing - Blurs/v1_Feb 2_Models-3 Images + Dataframe/Pred Images + OG/' + a[k].split('.')[0] + '_pred' + model.split('/')[6].split('.')[1].split('_')[0] +  model.split('.')[1].split('_')[-1] +'.jpg')\n","\n","for k in range(len(a)):\n","  filename = '/content/drive/Shareddrives/IDP/VisionAPI/IMAGES/SmartdocQA/' + a[k]\n","  image = Image.open(f\"{filename}\")\n","  image = image.rotate(-90)\n","  image.save('/content/drive/Shareddrives/IDP/Cycle GAN Implentation/9. Final OCR Testing - Blurs/v1_Feb 2_Models-3 Images + Dataframe/Pred Images + OG/' + a[k])\n"],"metadata":{"id":"HUOVe4MmSeoy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/Best_model_pred_OOF_v10_4033.zip /content/Best_model_pred_OOF_v10_4033"],"metadata":{"id":"yj-zYLHeebBS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#INTERPOLATION FOR BLACK EDGES\n","\n","filename = '/content/S_Img_Android_D28_L2_r35_a0_b0_Mb1_pred MB001411.jpg'\n","image1 = Image.open(f\"{filename}\")\n","#image = image1.rotate(-90)\n","image = np.asarray(image1)\n","output_shape = image.shape\n","patches = np.array(patchify(image, (256, 256, 3), step=256))\n","print(patches.shape)\n","output_patches = np.empty(patches.shape).astype(np.uint8)\n","\n","for i in range(patches.shape[0]):\n","  for j in range(patches.shape[1]):\n","    num = i * patches.shape[1] + j\n","    pixels=np.expand_dims(patches[i, j, 0], 0)\n","    pred = pixels.astype(np.int32)\n","    pred.setflags(write=1)\n","\n","    for x in range(11, 245):\n","      for y in range(10, -1,-1):\n","        pred[0][y][x] = (pred[0][y+1][x] + pred[0][y+5][x] + pred[0][y+3][x])/3\n","      for y in range(245, 256):\n","        pred[0][y][x] = (pred[0][y-4][x] + pred[0][y-5][x] + pred[0][y-3][x])/3\n","\n","    for y in range(0, 256):\n","      for x in range(10, -1,-1):\n","        pred[0][y][x] = (pred[0][y][x+4] + pred[0][y][x+5] + pred[0][y][x+3])/3\n","      for x in range(245, 256):\n","        pred[0][y][x] = (pred[0][y][x-4] + pred[0][y][x-5] + pred[0][y][x-3])/3\n","        \n","\n","    output_patches[i, j, 0] = pred\n","    \n","image_height, image_width, channel_count = output_shape\n","patch_height, patch_width, step = 256, 256, 256\n","\n","output_height = image_height - (image_height - patch_height) % step\n","output_width = image_width - (image_width - patch_width) % step\n","output_shape = (output_height, output_width, channel_count)\n","\n","output_image = unpatchify(output_patches, output_shape)\n","#pyplot.imshow((output_image))\n","output_image = Image.fromarray(output_image)\n","output_image.save('pred.jpg')"],"metadata":{"id":"HVy5gt2bXPpz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ONE IMAGE ONLY\n","model_list = ['/content/drive/Shareddrives/IDP/Cycle GAN Implentation/5. MB_Blur_Testing/Top_6_models_&_all_results_MB_only_v12/g_model_BtoA_001411.h5']\n","\n","net.eval()\n","with torch.no_grad():\n","\n"," for model in model_list:\n","   g_model1 = load_model(model,custom_objects={'InstanceNormalization':InstanceNormalization})\n","   filename = '/content/S_Img_Android_D26_L2_r35_a0_b0_Ob3_pred001411.jpg'\n","   image = Image.open(f\"{filename}\")\n","   image = image.rotate(-90)\n","   image = np.asarray(image)\n","   output_shape = image.shape\n","   patches = patchify(image, (256, 256, 3), step=256)\n","   print(patches.shape)\n","   output_patches = np.empty(patches.shape).astype(np.uint8)\n","   #fig, ax = plt.subplots(patches.shape[0], patches.shape[1],figsize=(20,20))\n","\n","   for i in range(patches.shape[0]):\n","    for j in range(patches.shape[1]):\n","      num = i * patches.shape[1] + j\n","      pixels=np.expand_dims(patches[i, j, 0], 0)\n","      output=net(torch.tensor(pixels/255,dtype=torch.float32).reshape(1,3,256,256))\n","      p = round(output.item(),2)\n","\n","      if p > 0.1:\n","        print('predicting')\n","          #print(pixels.max(),pixels.min())\n","        pred = 255*summarize_performance(f\"{'M_Img_Android_D21_L3_r35_a5_b10_Mb2'}patch_pred{num}.jpg\",1, g_model1, pixels, 'Mb2_blurB2A1', n_samples=1)\n","        pred = pred.astype(int)\n","          #print(pred.max(),pred.min())\n","      else:\n","        pred = pixels\n","        #print(pixels.max(),pixels.min())\n","        print('keeping same')\n","      output_patches[i, j, 0] = pred\n","\n","\n","    #from patchify import unpatchify\n","    image_height, image_width, channel_count = output_shape\n","    patch_height, patch_width, step = 256, 256, 256\n","\n","    output_height = image_height - (image_height - patch_height) % step\n","    output_width = image_width - (image_width - patch_width) % step\n","    output_shape = (output_height, output_width, channel_count)\n","\n","    output_image = unpatchify(output_patches, output_shape)\n","    #pyplot.imshow((output_image))\n","    output_image = Image.fromarray(output_image)\n","    output_image.save('/content/S_Img_Android_D26_L2_r35_a0_b0_Ob3_pred001411_pred.jpg')"],"metadata":{"id":"2dsOLbicXfI6"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}